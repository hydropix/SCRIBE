"""Configurable Ollama client with system prompts management"""

import json
import logging
import os
import re
from typing import Dict, Any, Optional, List
import ollama
from difflib import SequenceMatcher


class OllamaClient:
    """Client to interact with Ollama using configuration from packages"""

    def __init__(
        self,
        config: dict,
        prompts: dict = None,
        language: str = "English"
    ):
        """
        Initializes the Ollama client

        Args:
            config: Configuration dict (model, temperature, etc.)
            prompts: Prompts dict with system_prompts
            language: Language for report generation (default: English)
        """
        self.logger = logging.getLogger("SCRIBE.OllamaClient")
        self.config = config
        self.prompts = prompts.get('system_prompts', {}) if prompts else {}

        self.model = self.config.get('model', 'mistral')
        self.parameters = self.config.get('parameters', {})
        self.performance = self.config.get('performance', {})
        self.language = language

        # Configure Ollama host from .env
        self.ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')

        # Create Ollama client with configured host
        self.client = ollama.Client(host=self.ollama_host)

        self.logger.info(f"Ollama client initialized with model: {self.model} on {self.ollama_host} (language: {language})")
        self._verify_model()

    def _verify_model(self):
        """Verifies that the model is available in Ollama"""
        try:
            models_response = self.client.list()

            # Handle different response formats (dict or Pydantic object)
            model_list = []
            if isinstance(models_response, dict) and "models" in models_response:
                model_list = models_response["models"]
            elif hasattr(models_response, 'models'):
                model_list = models_response.models

            # Extract model names
            model_names = []
            for model in model_list:
                if isinstance(model, dict):
                    model_names.append(model.get('name') or model.get('model', ''))
                elif hasattr(model, 'model'):
                    model_names.append(model.model)
                else:
                    model_names.append(str(model))

            if not any(self.model in name for name in model_names):
                self.logger.warning(
                    f"Model '{self.model}' not found in Ollama. "
                    f"Available models: {model_names}. "
                    f"You may need to run: ollama pull {self.model}"
                )
        except Exception as e:
            self.logger.error(f"Error verifying model: {e}")

    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        Generates a response with the configured model

        Args:
            prompt: The user prompt
            system_prompt: The system prompt (optional)
            **kwargs: Additional parameters to pass to Ollama

        Returns:
            The response generated by the model
        """
        try:
            # Merge default parameters with provided ones
            options = {**self.parameters, **kwargs}

            # Prepare messages
            messages = []
            if system_prompt:
                messages.append({
                    'role': 'system',
                    'content': system_prompt
                })
            messages.append({
                'role': 'user',
                'content': prompt
            })

            self.logger.debug(f"Generating response with {self.model}")

            # Call Ollama via configured client
            response = self.client.chat(
                model=self.model,
                messages=messages,
                options=options
            )

            return response['message']['content']

        except Exception as e:
            self.logger.error(f"Error generating response: {e}")
            raise

    def analyze_relevance(self, content: str, title: str = "", metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Analyzes the relevance of content for AI monitoring

        Args:
            content: The content to analyze
            title: The title (optional)
            metadata: Additional metadata (score, upvote_ratio, num_comments, etc.)

        Returns:
            Dictionary with relevant, score, reason, category
        """
        system_prompt = self.prompts.get('relevance_analyzer', '')

        # Build user prompt with metadata context
        metadata = metadata or {}

        # Build metadata section for context
        metadata_lines = []

        # Reddit-specific metadata
        if metadata.get('source') == 'reddit':
            if 'score' in metadata:
                metadata_lines.append(f"Upvotes: {metadata['score']}")
            if 'upvote_ratio' in metadata:
                ratio_percent = int(metadata['upvote_ratio'] * 100)
                metadata_lines.append(f"Upvote ratio: {ratio_percent}%")
            if 'num_comments' in metadata:
                metadata_lines.append(f"Comments: {metadata['num_comments']}")
            if 'subreddit' in metadata:
                metadata_lines.append(f"Subreddit: r/{metadata['subreddit']}")
            if 'author' in metadata:
                metadata_lines.append(f"Author: u/{metadata['author']}")

        # YouTube-specific metadata
        elif metadata.get('source') == 'youtube':
            if 'channel' in metadata:
                metadata_lines.append(f"Channel: {metadata['channel']}")
            if 'view_count' in metadata:
                metadata_lines.append(f"Views: {metadata['view_count']}")

        # Build the prompt
        if metadata_lines:
            metadata_str = "\n".join(metadata_lines)
            user_prompt = f"Titre: {title}\n\n{metadata_str}\n\nContenu:\n{content[:3000]}"
        else:
            user_prompt = f"Titre: {title}\n\nContenu:\n{content[:3000]}"  # Limit to 3000 chars

        try:
            response = self.generate(user_prompt, system_prompt)

            # Parse JSON response
            # Clean the response (sometimes the model adds text before/after JSON)
            response = response.strip()
            if '```json' in response:
                response = response.split('```json')[1].split('```')[0].strip()
            elif '```' in response:
                response = response.split('```')[1].split('```')[0].strip()

            result = json.loads(response)

            # Validate structure
            required_keys = ['pertinent', 'score', 'raison', 'categorie']
            if not all(key in result for key in required_keys):
                self.logger.warning(f"Incomplete response structure: {result}")
                return {
                    'pertinent': False,
                    'score': 0,
                    'raison': 'Erreur de parsing',
                    'categorie': 'Unknown'
                }

            return result

        except json.JSONDecodeError as e:
            self.logger.error(f"Error parsing JSON response: {e}\nResponse: {response}")
            return {
                'pertinent': False,
                'score': 0,
                'raison': f'Erreur JSON: {str(e)}',
                'categorie': 'Unknown'
            }
        except Exception as e:
            self.logger.error(f"Error analyzing relevance: {e}")
            return {
                'pertinent': False,
                'score': 0,
                'raison': f'Erreur: {str(e)}',
                'categorie': 'Unknown'
            }

    def extract_insights(self, content: str, title: str = "") -> Dict[str, Any]:
        """
        Extracts key insights from content

        Args:
            content: The content to analyze
            title: The title (optional)

        Returns:
            Dictionary with translated_title, hook, and insights
        """
        system_prompt = self.prompts.get('insight_extractor', '').format(language=self.language)

        user_prompt = f"Titre: {title}\n\nContenu:\n{content[:4000]}"

        try:
            response = self.generate(user_prompt, system_prompt)

            # Parse JSON response
            response = response.strip()
            if '```json' in response:
                response = response.split('```json')[1].split('```')[0].strip()
            elif '```' in response:
                response = response.split('```')[1].split('```')[0].strip()

            result = json.loads(response)

            # Validate structure
            required_keys = ['translated_title', 'hook', 'insights']
            if not all(key in result for key in required_keys):
                self.logger.warning(f"Incomplete insight response structure: {result}")
                return {
                    'translated_title': title,
                    'hook': '',
                    'insights': str(result)
                }

            return result

        except json.JSONDecodeError as e:
            self.logger.error(f"Error parsing JSON insights response: {e}\nResponse: {response}")
            # Fallback: return the raw response as insights
            return {
                'translated_title': title,
                'hook': '',
                'insights': response if 'response' in locals() else f"Erreur JSON: {str(e)}"
            }
        except Exception as e:
            self.logger.error(f"Error extracting insights: {e}")
            return {
                'translated_title': title,
                'hook': '',
                'insights': f"Erreur lors de l'extraction des insights: {str(e)}"
            }

    def generate_executive_summary(self, insights: List[str]) -> str:
        """
        Generates an executive summary from a list of insights

        Args:
            insights: List of insights to summarize

        Returns:
            The executive summary in Markdown format
        """
        system_prompt = self.prompts.get('executive_summary', '').format(language=self.language)

        user_prompt = "Insights à résumer:\n\n" + "\n\n---\n\n".join(insights)

        try:
            summary = self.generate(user_prompt, system_prompt)
            return summary
        except Exception as e:
            self.logger.error(f"Error generating executive summary: {e}")
            return f"Erreur lors de la génération du résumé: {str(e)}"

    def check_similarity(self, content1: str, content2: str) -> tuple[str, str]:
        """
        Checks similarity between two contents

        Args:
            content1: First content
            content2: Second content

        Returns:
            Tuple (level, explanation) where level = IDENTIQUE|SIMILAIRE|DIFFERENT
        """
        system_prompt = self.prompts.get('similarity_checker', '')

        user_prompt = f"""Contenu 1:
{content1[:1500]}

Contenu 2:
{content2[:1500]}"""

        try:
            response = self.generate(user_prompt, system_prompt)
            lines = response.strip().split('\n', 1)

            level = lines[0].strip().upper()
            explanation = lines[1].strip() if len(lines) > 1 else "No explanation"

            # Validate level
            if level not in ['IDENTIQUE', 'SIMILAIRE', 'DIFFERENT']:
                self.logger.warning(f"Invalid similarity level: {level}")
                level = 'DIFFERENT'

            return level, explanation

        except Exception as e:
            self.logger.error(f"Error checking similarity: {e}")
            return 'DIFFERENT', f"Erreur: {str(e)}"

    def _inject_links_in_summary(self, summary: str, relevant_contents: List[Dict[str, Any]]) -> str:
        """
        Injects source links into bold text in the summary by matching with content titles

        Args:
            summary: The generated summary text with bold markdown
            relevant_contents: List of content items with titles and URLs

        Returns:
            Summary with bold text converted to linked bold text
        """
        # Find all bold text patterns: **text**
        bold_pattern = re.compile(r'\*\*([^\*]+?)\*\*')
        bold_matches = bold_pattern.findall(summary)

        if not bold_matches:
            return summary

        # Build a mapping of titles to URLs
        title_url_map = {}
        for content in relevant_contents:
            title = content.get('translated_title', content.get('title', ''))
            url = content.get('url', '')
            if title and url:
                title_url_map[title.lower()] = url

        # Process each bold text and try to match with a title
        result = summary
        for bold_text in bold_matches:
            best_match = None
            best_ratio = 0.0
            bold_lower = bold_text.lower()

            # Try to find the best matching title
            for title, url in title_url_map.items():
                # Use fuzzy matching to handle variations
                ratio = SequenceMatcher(None, bold_lower, title).ratio()

                # Also check if bold text is contained in title or vice versa
                if bold_lower in title or title in bold_lower:
                    ratio = max(ratio, 0.85)  # Boost ratio for substring matches

                if ratio > best_ratio and ratio > 0.6:  # Threshold of 60% similarity
                    best_ratio = ratio
                    best_match = url

            # If we found a good match, replace bold with linked bold
            if best_match:
                old_pattern = f'**{bold_text}**'
                new_pattern = f'[**{bold_text}**]({best_match})'
                # Only replace the first occurrence to avoid issues
                result = result.replace(old_pattern, new_pattern, 1)
                self.logger.debug(f"Linked '{bold_text}' to {best_match} (similarity: {best_ratio:.2f})")

        return result

    def generate_daily_summary(self, relevant_contents: List[Dict[str, Any]]) -> str:
        """
        Generates a daily summary for Discord (will be split if > 2000 chars)

        Args:
            relevant_contents: List of analyzed content items with insights

        Returns:
            A summary string formatted for Discord
        """
        system_prompt = self.prompts.get('daily_summary', '').format(
            language=self.language
        )

        # Prepare content for summarization
        insights_text = []
        for i, content in enumerate(relevant_contents[:20], 1):  # Limit to 20 items
            title = content.get('translated_title', content.get('title', 'Untitled'))
            category = content.get('category', 'Other')
            hook = content.get('hook', '')

            # Build compact representation
            item = f"{i}. [{category}] {title}"
            if hook:
                item += f" - {hook}"
            insights_text.append(item)

        user_prompt = f"""Total insights: {len(relevant_contents)}

Key insights to summarize:
{chr(10).join(insights_text)}"""

        try:
            summary = self.generate(user_prompt, system_prompt)

            # Inject links into bold text in the summary
            summary_with_links = self._inject_links_in_summary(summary, relevant_contents)

            return summary_with_links

        except Exception as e:
            self.logger.error(f"Error generating daily summary: {e}")
            # Fallback to a simple summary
            return f"**Daily AI Summary**\n\n{len(relevant_contents)} insights gathered today across multiple AI topics.\n\nError generating detailed summary: {str(e)}"


if __name__ == "__main__":
    # Quick test
    from pathlib import Path
    import sys
    sys.path.append(str(Path(__file__).parent.parent.parent))

    logging.basicConfig(level=logging.INFO)

    from src.package_manager import PackageManager

    # Load config from package
    pm = PackageManager()
    pkg = pm.load_package("ai_trends")

    client = OllamaClient(
        config=pkg.get_ollama_config(),
        prompts=pkg.prompts
    )

    # Relevance analysis test
    test_content = """
    Une nouvelle étude montre que GPT-5 pourrait avoir des capacités de raisonnement
    significativement améliorées par rapport à GPT-4. Les chercheurs ont observé
    une amélioration de 40% sur les benchmarks de raisonnement logique.
    """

    result = client.analyze_relevance(test_content, "GPT-5: Amélioration du raisonnement")
    print("Test analyse de pertinence:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

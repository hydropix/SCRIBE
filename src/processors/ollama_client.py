"""Configurable Ollama client with system prompts management"""

import json
import logging
import os
from typing import Dict, Any, Optional, List
import ollama
from pathlib import Path
import sys

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.utils import load_config


class OllamaClient:
    """Client to interact with Ollama using YAML configuration"""

    def __init__(self, config_path: str = "config/ollama_config.yaml"):
        """
        Initializes the Ollama client

        Args:
            config_path: Path to the configuration file
        """
        self.logger = logging.getLogger("SCRIBE.OllamaClient")
        self.config = load_config(config_path)

        self.model = self.config.get('model', 'mistral')
        self.parameters = self.config.get('parameters', {})
        self.prompts = self.config.get('prompts', {})
        self.performance = self.config.get('performance', {})

        # Configure Ollama host from .env
        self.ollama_host = os.getenv('OLLAMA_HOST', 'http://localhost:11434')

        # Create Ollama client with configured host
        self.client = ollama.Client(host=self.ollama_host)

        self.logger.info(f"Ollama client initialized with model: {self.model} on {self.ollama_host}")
        self._verify_model()

    def _verify_model(self):
        """Verifies that the model is available in Ollama"""
        try:
            models = self.client.list()
            model_names = [model.get('name') or model.get('model', '') for model in models.get('models', [])]

            if not any(self.model in name for name in model_names):
                self.logger.warning(
                    f"Model '{self.model}' not found in Ollama. "
                    f"Available models: {model_names}. "
                    f"You may need to run: ollama pull {self.model}"
                )
        except Exception as e:
            self.logger.error(f"Error verifying model: {e}")

    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        Generates a response with the configured model

        Args:
            prompt: The user prompt
            system_prompt: The system prompt (optional)
            **kwargs: Additional parameters to pass to Ollama

        Returns:
            The response generated by the model
        """
        try:
            # Merge default parameters with provided ones
            options = {**self.parameters, **kwargs}

            # Prepare messages
            messages = []
            if system_prompt:
                messages.append({
                    'role': 'system',
                    'content': system_prompt
                })
            messages.append({
                'role': 'user',
                'content': prompt
            })

            self.logger.debug(f"Generating response with {self.model}")

            # Call Ollama via configured client
            response = self.client.chat(
                model=self.model,
                messages=messages,
                options=options
            )

            return response['message']['content']

        except Exception as e:
            self.logger.error(f"Error generating response: {e}")
            raise

    def analyze_relevance(self, content: str, title: str = "") -> Dict[str, Any]:
        """
        Analyzes the relevance of content for AI monitoring

        Args:
            content: The content to analyze
            title: The title (optional)

        Returns:
            Dictionary with relevant, score, reason, category
        """
        system_prompt = self.prompts.get('relevance_analyzer', '')

        user_prompt = f"Titre: {title}\n\nContenu:\n{content[:3000]}"  # Limit to 3000 chars

        try:
            response = self.generate(user_prompt, system_prompt)

            # Parse JSON response
            # Clean the response (sometimes the model adds text before/after JSON)
            response = response.strip()
            if '```json' in response:
                response = response.split('```json')[1].split('```')[0].strip()
            elif '```' in response:
                response = response.split('```')[1].split('```')[0].strip()

            result = json.loads(response)

            # Validate structure
            required_keys = ['pertinent', 'score', 'raison', 'categorie']
            if not all(key in result for key in required_keys):
                self.logger.warning(f"Incomplete response structure: {result}")
                return {
                    'pertinent': False,
                    'score': 0,
                    'raison': 'Erreur de parsing',
                    'categorie': 'Unknown'
                }

            return result

        except json.JSONDecodeError as e:
            self.logger.error(f"Error parsing JSON response: {e}\nResponse: {response}")
            return {
                'pertinent': False,
                'score': 0,
                'raison': f'Erreur JSON: {str(e)}',
                'categorie': 'Unknown'
            }
        except Exception as e:
            self.logger.error(f"Error analyzing relevance: {e}")
            return {
                'pertinent': False,
                'score': 0,
                'raison': f'Erreur: {str(e)}',
                'categorie': 'Unknown'
            }

    def extract_insights(self, content: str, title: str = "") -> str:
        """
        Extracts key insights from content

        Args:
            content: The content to analyze
            title: The title (optional)

        Returns:
            The insights in Markdown format
        """
        system_prompt = self.prompts.get('insight_extractor', '')

        user_prompt = f"Titre: {title}\n\nContenu:\n{content[:4000]}"

        try:
            insights = self.generate(user_prompt, system_prompt)
            return insights
        except Exception as e:
            self.logger.error(f"Error extracting insights: {e}")
            return f"Erreur lors de l'extraction des insights: {str(e)}"

    def generate_executive_summary(self, insights: List[str]) -> str:
        """
        Generates an executive summary from a list of insights

        Args:
            insights: List of insights to summarize

        Returns:
            The executive summary in Markdown format
        """
        system_prompt = self.prompts.get('executive_summary', '')

        user_prompt = "Insights à résumer:\n\n" + "\n\n---\n\n".join(insights)

        try:
            summary = self.generate(user_prompt, system_prompt)
            return summary
        except Exception as e:
            self.logger.error(f"Error generating executive summary: {e}")
            return f"Erreur lors de la génération du résumé: {str(e)}"

    def check_similarity(self, content1: str, content2: str) -> tuple[str, str]:
        """
        Checks similarity between two contents

        Args:
            content1: First content
            content2: Second content

        Returns:
            Tuple (level, explanation) where level = IDENTIQUE|SIMILAIRE|DIFFERENT
        """
        system_prompt = self.prompts.get('similarity_checker', '')

        user_prompt = f"""Contenu 1:
{content1[:1500]}

Contenu 2:
{content2[:1500]}"""

        try:
            response = self.generate(user_prompt, system_prompt)
            lines = response.strip().split('\n', 1)

            level = lines[0].strip().upper()
            explanation = lines[1].strip() if len(lines) > 1 else "No explanation"

            # Validate level
            if level not in ['IDENTIQUE', 'SIMILAIRE', 'DIFFERENT']:
                self.logger.warning(f"Invalid similarity level: {level}")
                level = 'DIFFERENT'

            return level, explanation

        except Exception as e:
            self.logger.error(f"Error checking similarity: {e}")
            return 'DIFFERENT', f"Erreur: {str(e)}"


if __name__ == "__main__":
    # Quick test
    logging.basicConfig(level=logging.INFO)

    client = OllamaClient()

    # Relevance analysis test
    test_content = """
    Une nouvelle étude montre que GPT-5 pourrait avoir des capacités de raisonnement
    significativement améliorées par rapport à GPT-4. Les chercheurs ont observé
    une amélioration de 40% sur les benchmarks de raisonnement logique.
    """

    result = client.analyze_relevance(test_content, "GPT-5: Amélioration du raisonnement")
    print("Test analyse de pertinence:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
